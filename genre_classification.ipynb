
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
nltk.download('stopwords')
import re
from nltk.corpus import stopwords
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

train_data = pd.read_csv('/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/train_data.txt', sep=' ::: ', engine='python', header = None, names =  ['title','genre','description'])
test_data = pd.read_csv('/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/test_data_solution.txt', sep=' ::: ', engine='python', header=None, names =  ['title','genre','description'])
test_data_solution = pd.read_csv('/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/test_data_solution.txt', sep=' ::: ', engine='python', header=None, names =  ['title','genre','description'])

plt.figure(figsize=(14,7)) 
counts = train_data.genre.value_counts() 
sns.barplot(x=counts, y=counts.index, orient='h') 
plt.xlabel('Genre') 
plt.ylabel('Count')

train_data['feature'] = train_data['title'] + ' ' + train_data['description']
test_data['feature'] = test_data[['title', 'description']].agg(' '.join, axis = 1)

# Initialize TF-IDF Vectorizer with English stop words removal

vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 5000)
x_train = vectorizer.fit_transform(train_data['feature'])
x_test = vectorizer.transform(test_data['feature'])

# Initialize LabelEncoder
label_encoder = LabelEncoder()

train_data['genre_encoded'] = label_encoder.fit_transform(train_data['genre'])
test_data['genre_encoded'] = label_encoder.transform(test_data['genre'])

# Assign encoded labels as target variables for model training
y_train = train_data['genre_encoded']
y_test = test_data['genre_encoded']

model = MultinomialNB(alpha = 0.2, force_alpha = True)

model.fit(x_train,y_train)

y_pred = model.predict(x_train)

accuracy = accuracy_score(y_pred,y_train)
print(accuracy)

y_test_pred = model.predict(x_test)
accuracy = accuracy_score(y_test_pred, y_test)
print(accuracy)

cr = classification_report(y_test_pred, y_test)
print(cr)

import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

# Initialize a sequential model
model = Sequential()

# Input layer with 5000 features (TF-IDF vector size) and 128 neurons with ReLU activation
model.add(Dense(128, activation = 'relu', input_dim = 5000))
model.add(Dense(64, activation = 'relu'))
model.add(Dense(27, activation = 'softmax'))

model.summary()

# Compile the model with Adam optimizer and Sparse Categorical Crossentropy loss
model.compile(optimizer = 'Adam', 
              loss = 'SparseCategoricalCrossentropy',
              metrics = ['accuracy']
             )

# Convert the sparse TF-IDF matrix to a dense NumPy array and then to a TensorFlow tensor
x_train = tf.sparse.from_dense(x_train.toarray()) 
x_test = tf.sparse.from_dense(x_test.toarray()) 

# Train the neural network model
history = model.fit(
    x_train,       # Input training data (TF-IDF features as tensors)
    y_train,       # Target labels (integer-encoded genres)
    epochs=10,     # Number of times the model will see the entire dataset
    batch_size=5120  # Number of samples per batch for training
)

# Predict genre labels using the trained deep learning model
y_pred_dl = model.predict(x_test)  

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(x_test, y_test, batch_size=5120)

# Print test accuracy and loss
print(f"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")

# Convert predicted probabilities to class labels
y_pred = np.argmax(y_pred_dl, axis=1)  

# Generate classification report
cr_dl = classification_report(y_test, y_pred)  

# Print the classification report
print(cr_dl)
